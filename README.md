# RAG Semantic Search with Hugging Face and MongoDB

This project implements a **Retrieval-Augmented Generation (RAG)** semantic search system leveraging powerful techniques in natural language processing (NLP) to enhance search relevance and accuracy. By incorporating **Generative AI** and **Large Language Models (LLMs)**, this solution not only retrieves the most relevant documents but also generates contextually rich and meaningful responses.

## Why Use RAG?

**Retrieval-Augmented Generation (RAG)** is a cutting-edge approach that combines **retrieval-based search** and **generation** techniques to provide highly relevant and contextual results. RAG improves traditional search systems by seamlessly integrating **Generative AI**, specifically using **Large Language Models (LLMs)**, to generate detailed, context-aware responses based on retrieved data. This combination makes RAG an ideal choice for applications requiring deep understanding and dynamic response generation.

### Key Benefits of RAG:
1. **Contextual Understanding with LLMs**: Unlike traditional keyword-based search, RAG utilizes **LLMs** to understand the deeper semantic meaning of both the query and the documents, improving relevance.
2. **Generative Responses**: After retrieving the most relevant documents, the system uses **Generative AI** to craft responses that go beyond simple document retrieval, offering more meaningful and personalized results.
3. **Efficient Search and Generation**: The combination of **vector search** and **generative models** allows for efficient retrieval of information and generation of content that truly aligns with the user’s intent.

## Key Technical Details

- **Hugging Face Embeddings**: We utilize Hugging Face's feature extraction API to generate high-quality, context-aware embeddings from textual data. The embeddings represent the semantic meaning of each text, enabling the model to compare, search, and retrieve information based on deep linguistic understanding. This process is powered by **transformer-based LLMs**, making the system capable of complex semantic tasks.

- **MongoDB Vector Search**: MongoDB serves as the database for storing both the raw data and its embeddings. By utilizing MongoDB’s vector search capabilities (via `knnVector`), we can efficiently search for documents based on their semantic similarity. The embeddings are indexed to provide fast and scalable similarity searches, enabling the system to retrieve documents that best match the query’s context.

- **Vector Index**: To enable efficient search, we create a **vector index** on the `plot_embedding_hf` field. This index allows us to perform high-performance similarity searches using the `dotProduct` similarity measure, ensuring that the most relevant documents are returned based on their proximity in the embedding space.

    **MongoDB Index Mapping Example**:
    ```json
    {
      "mappings": {
        "dynamic": true,
        "fields": {
          "plot_embedding_hf": [
            {
              "dimensions": 384,
              "similarity": "dotProduct",
              "type": "knnVector"
            }
          ]
        }
      }
    }
    ```

- **Embedding Process**: The system generates embeddings for text fields (e.g., movie plots) and stores them in MongoDB. This process allows us to continuously enrich the dataset with meaningful vector representations, which are then used for fast and accurate search queries.

## Why This Approach?

- **Generative AI at the Core**: The use of **LLMs** enables the system to not only retrieve information but also generate insightful, dynamic responses. This is key for applications like conversational AI, document summarization, and personalized recommendations.
  
- **Contextual Search**: By using embeddings generated by **LLMs** like BERT, GPT, or T5, the system goes beyond mere keyword matching and understands the underlying meaning of the text. This results in significantly more relevant search results and a more nuanced understanding of user queries.

- **Scalability and Flexibility**: MongoDB's built-in vector search capabilities allow the solution to scale as the dataset grows. With vector indexing and a robust database structure, this approach is ideal for real-time applications where speed and accuracy are crucial.

- **State-of-the-Art NLP**: By leveraging the latest **Generative AI** models and **transformer-based LLMs**, the system ensures that semantic understanding and generation are at the cutting edge of NLP technology.

## Use Cases

This architecture is particularly well-suited for a variety of modern applications, including:

- **Content Discovery**: Helping users discover content that aligns with their interests by searching for semantically related topics, articles, or media, powered by **Generative AI**.
- **Knowledge Base Systems**: Allowing users to query knowledge bases and get contextually relevant answers, even when the wording of the query doesn’t exactly match the documents.
- **Personalized Recommendations**: Tailoring search results to the user’s preferences and context by understanding the deeper meaning behind their queries, made possible by **LLMs**.

---

This setup brings together **Retrieval-Augmented Generation (RAG)**, **Generative AI**, and **Large Language Models (LLMs)** to create a powerful, intelligent system that provides highly relevant and context-aware search results. With **semantic search** and **vector indexing** at its core, this project demonstrates the true potential of modern NLP and generative technologies in real-world applications.

<img width="1256" alt="Screenshot 2025-02-04 at 1 42 50 AM" src="https://github.com/user-attachments/assets/fe2db581-6c4d-4525-8d46-dc2c12403bae" />

